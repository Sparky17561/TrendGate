"""
Test Script for Additional Metrics Integration
================================================
Tests Google Trends and Reddit scraping functionality
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_google_trends():
    """Test Google Trends helper."""
    print("\n" + "="*60)
    print("Testing Google Trends Integration")
    print("="*60)
    
    try:
        from trendguard.utils import fetch_google_trends_metrics, analyze_trends_decline_risk
        
        print("\n‚úì Imports successful")
        
        # Test with a popular trend
        trend = "artificial intelligence"
        print(f"\nüìä Fetching Google Trends data for: {trend}")
        
        metrics = fetch_google_trends_metrics(trend)
        
        if metrics:
            print(f"\n‚úì Data retrieved successfully!")
            print(f"  ‚Ä¢ Direction: {metrics['direction']}")
            print(f"  ‚Ä¢ Current Interest: {metrics['current_value']:.1f}")
            print(f"  ‚Ä¢ Slope: {metrics['slope']:.2f}")
            print(f"  ‚Ä¢ Data Points: {metrics['data_points']}")
            
            # Analyze risk
            risk = analyze_trends_decline_risk(metrics)
            print(f"\nüìà Risk Analysis:")
            print(f"  ‚Ä¢ Risk Level: {risk['risk_level']}")
            print(f"  ‚Ä¢ Risk Score: {risk['risk_score']}")
            print(f"  ‚Ä¢ Recommendation: {risk['recommendation']}")
            
            if risk['signals']:
                print(f"  ‚Ä¢ Signals: {', '.join(risk['signals'])}")
            
            return True
        else:
            print("‚ö†Ô∏è No data returned (this is OK if pytrends isn't installed)")
            return False
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_reddit_scraping():
    """Test Reddit scraper."""
    print("\n" + "="*60)
    print("Testing Reddit Scraping")
    print("="*60)
    
    try:
        from trendguard.utils import scrape_subreddit, aggregate_reddit_metrics, analyze_reddit_decline_risk
        
        print("\n‚úì Imports successful")
        
        # Test with a small subreddit
        subreddit = "python"
        limit = 20
        
        print(f"\nüîç Scraping r/{subreddit} (limit: {limit} posts)")
        print("‚è≥ This may take a few seconds...")
        
        posts = scrape_subreddit(subreddit, limit=limit, delay=0.5)
        
        if posts:
            print(f"\n‚úì Scraped {len(posts)} posts!")
            
            # Show sample post
            if len(posts) > 0:
                sample = posts[0]
                print(f"\nüìù Sample Post:")
                print(f"  ‚Ä¢ Title: {sample['title'][:60]}...")
                print(f"  ‚Ä¢ Score: {sample['score']}")
                print(f"  ‚Ä¢ Comments: {sample['comments']}")
                print(f"  ‚Ä¢ Engagement: {sample['engagement']}")
                print(f"  ‚Ä¢ Sentiment: {sample['sentiment']}")
            
            # Aggregate metrics
            metrics = aggregate_reddit_metrics(posts, window_days=30)
            print(f"\nüìä Aggregated Metrics:")
            print(f"  ‚Ä¢ Avg Engagement: {metrics['avg_engagement']:.1f}")
            print(f"  ‚Ä¢ Engagement Velocity: {metrics['engagement_velocity']:.2f}")
            print(f"  ‚Ä¢ Post Velocity: {metrics['post_velocity']:.2f}")
            print(f"  ‚Ä¢ Sentiment Shift: {metrics['sentiment_shift']:.2f}")
            print(f"  ‚Ä¢ Current Posts: {metrics['current_posts']}")
            
            # Analyze risk
            risk = analyze_reddit_decline_risk(metrics)
            print(f"\nüìà Risk Analysis:")
            print(f"  ‚Ä¢ Risk Level: {risk['risk_level']}")
            print(f"  ‚Ä¢ Risk Score: {risk['risk_score']}")
            print(f"  ‚Ä¢ Recommendation: {risk['recommendation']}")
            
            if risk['signals']:
                print(f"  ‚Ä¢ Signals:")
                for signal in risk['signals']:
                    print(f"    - {signal}")
            
            return True
        else:
            print("‚ö†Ô∏è No posts scraped")
            return False
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_gemini_integration():
    """Test Gemini advisor with additional metrics."""
    print("\n" + "="*60)
    print("Testing Gemini Advisor Integration")
    print("="*60)
    
    try:
        # Check if Gemini API key is available
        from dotenv import load_dotenv
        load_dotenv()
        
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("\n‚ö†Ô∏è GEMINI_API_KEY not found - skipping Gemini test")
            print("   (This is OK for testing just the data collection)")
            return False
        
        from trendguard.gemini_advisor import CampaignAdvisor
        
        print("\n‚úì Imports successful")
        print("\nü§ñ Initializing Gemini advisor...")
        
        advisor = CampaignAdvisor()
        
        print("\nüìä Running health check (this includes additional metrics)...")
        print("‚è≥ This may take 10-15 seconds...")
        
        result = advisor.check_trend_health("sustainable fashion")
        
        if "error" in result:
            print(f"‚ùå Error in analysis: {result['error']}")
            return False
        
        print(f"\n‚úì Analysis complete!")
        
        # Check if additional metrics are present
        if "additional_metrics" in result:
            print(f"\n‚úÖ Additional metrics successfully integrated!")
            
            am = result["additional_metrics"]
            
            if am.get("google_trends"):
                gt = am["google_trends"]
                if "metrics" in gt:
                    print(f"\n  üìà Google Trends:")
                    print(f"    ‚Ä¢ Direction: {gt['metrics']['direction']}")
                    print(f"    ‚Ä¢ Risk Level: {gt['risk_analysis']['risk_level']}")
            
            if am.get("reddit"):
                rd = am["reddit"]
                if "metrics" in rd:
                    print(f"\n  üí¨ Reddit:")
                    print(f"    ‚Ä¢ Subreddits: {', '.join(rd['subreddits_analyzed'])}")
                    print(f"    ‚Ä¢ Total Posts: {rd['total_posts']}")
                    print(f"    ‚Ä¢ Risk Level: {rd['risk_analysis']['risk_level']}")
        else:
            print(f"\n‚ö†Ô∏è Additional metrics not found in result")
            print(f"   Available keys: {list(result.keys())}")
        
        return True
        
    except ImportError as e:
        print(f"\n‚ö†Ô∏è Import error (this is OK if dependencies aren't installed yet): {e}")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("\n" + "="*60)
    print("TrendGuard Additional Metrics Test Suite")
    print("="*60)
    
    results = {
        "Google Trends": test_google_trends(),
        "Reddit Scraping": test_reddit_scraping(),
        "Gemini Integration": test_gemini_integration()
    }
    
    print("\n" + "="*60)
    print("Test Summary")
    print("="*60)
    
    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ö†Ô∏è SKIP/FAIL"
        print(f"{status} - {test_name}")
    
    print("\n" + "="*60)
    
    if all(results.values()):
        print("üéâ All tests passed!")
    elif any(results.values()):
        print("‚ö†Ô∏è Some tests passed - check failures above")
    else:
        print("‚ùå All tests failed - check errors above")
    
    print("="*60 + "\n")
